{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and serving Tensorflow Estimators\n",
    "This notebook shows step by step how to generate input data, train and evaluate a Convolutional Neural Network model, output results, save models, and make predictions using the high-level [Estimator API](https://www.tensorflow.org/guide/estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "BATCH_SIZE = 100                      # number of samples per batch\n",
    "MAX_STEPS = 1000                      # max number of training steps\n",
    "EVAL_STEPS = 1                        # number of steps to run evaluation\n",
    "\n",
    "SAVE_SUMMARY_STEPS = 100              # frequency of summary saving\n",
    "SAVE_CHECKPOINTS_STEPS = 200          # frequency of checkpoint saving - also correspond to evaluation frequency\n",
    "LOGGING_STEPS = 50                    # frequency of logging output\n",
    "\n",
    "MODEL_DIR = '../models/ckpt/'         # save model and checkpoints\n",
    "SAVE_DIR = '../models/pb/'            # save model for TF serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape = (60000, 28, 28)\n",
      "train labels shape = (60000,)\n",
      "evaluation data shape = (10000, 28, 28)\n",
      "evaluation labels shape = (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load training and eval data\n",
    "((train_data, train_labels),\n",
    " (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data/np.float32(255)\n",
    "train_labels = train_labels.astype(np.int32)  # not required\n",
    "\n",
    "eval_data = eval_data/np.float32(255)\n",
    "eval_labels = eval_labels.astype(np.int32)  # not required\n",
    "\n",
    "print('train data shape =', train_data.shape)\n",
    "print('train labels shape =', train_labels.shape)\n",
    "print('evaluation data shape =', eval_data.shape)\n",
    "print('evaluation labels shape =', eval_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Input Data\n",
    "To load data into Estimator `input_fn()` is created through the `tf.data.Dataset.from_tensor_slices` API, which allows to feed numpy arrays into the model function. In this case trainining features are returned by `input_fn()` as a dictionary `{'x': features}`, where `features` is a batch of image numpy arrays, instead of passing directly the array `features` to the model. This is necessary because during serving the serialized input must be in the form `{'key': feature}`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28) (100,)\n",
      "(100, 28, 28) (100,)\n",
      "(100, 28, 28) (100,)\n",
      "(100, 28, 28) (100,)\n",
      "(100, 28, 28) (100,)\n"
     ]
    }
   ],
   "source": [
    "def input_fn(data, labels, repeat=1):\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "    dataset = dataset.repeat(repeat) # None\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    features, labels = dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    return {'x':features}, labels\n",
    "\n",
    "# verify that the output of input_fn is correct\n",
    "features_batch, labels_batch = input_fn(train_data, train_labels)\n",
    "with tf.Session() as sess:\n",
    "    for i in range(5):\n",
    "        features, labels = sess.run([features_batch, labels_batch]) \n",
    "        print(features['x'].shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During prediction mode the data is not feed in batches but rather as single images/arrays. `predict_input_fn()` assumes that a single array of size 28x28 is to be classified, which needs to be reshape in the same format returned by `input_fn()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input_fn(predict_data):\n",
    "    \n",
    "    predict_data = tf.reshape(predict_data, [-1,28,28])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((predict_data))\n",
    "    dataset = dataset.repeat(None)\n",
    "    dataset = dataset.batch(1)\n",
    "    features = dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    return {'x':features}\n",
    "\n",
    "# verify that the output of predict_input_fn is correct\n",
    "predict_data = eval_data[0]        # sample of image to classify\n",
    "features_pred = predict_input_fn(predict_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    feature_pred = sess.run(features_pred)\n",
    "    print(feature_pred['x'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Custom DNN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    \n",
    "    input_layer = tf.reshape(features['x'], [-1, 28, 28, 1])\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense, rate=params['dropout_rate'], training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "    predictions = {\"classes\": tf.argmax(input=logits, axis=1), \"probabilities\": tf.nn.softmax(logits)}\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=params['learning_rate'])\n",
    "    \n",
    "    \n",
    "    loss = train_op = eval_metric_ops = None\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        \n",
    "        logging_hook = tf.train.LoggingTensorHook({'predictions': predictions['classes']}, \n",
    "                                                  every_n_iter=LOGGING_STEPS)    \n",
    "    \n",
    "    if (mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL):\n",
    "        \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "        accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "        tf.summary.scalar('accuracy', accuracy[1])\n",
    "        eval_metric_ops = {\"accuracy\": accuracy}\n",
    "        \n",
    "        logging_hook = tf.train.LoggingTensorHook({'loss': loss, 'accuracy': accuracy[1]}, \n",
    "                                                  every_n_iter=LOGGING_STEPS)\n",
    "        \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        \n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    estimator_spec = tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, loss=loss, train_op=train_op, \n",
    "                                                eval_metric_ops=eval_metric_ops, training_hooks=[logging_hook], \n",
    "                                                prediction_hooks=[logging_hook])\n",
    "        \n",
    "    return estimator_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = tf.estimator.RunConfig(\n",
    "    model_dir=MODEL_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, \n",
    "    model_dir=MODEL_DIR,\n",
    "    config=training_config,\n",
    "    params={\n",
    "        'dropout_rate': 0.4,\n",
    "        'learning_rate': 0.001\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = np.floor(MAX_STEPS/(len(train_data)/BATCH_SIZE)).astype(int)\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=lambda:input_fn(train_data, train_labels),\n",
    "    max_steps=MAX_STEPS)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=lambda:input_fn(eval_data, eval_labels),\n",
    "    steps=EVAL_STEPS,\n",
    "    name='validation',\n",
    "    start_delay_secs=10,\n",
    "    throttle_secs=20)\n",
    "\n",
    "tf.logging.info('start experiment...')\n",
    "tf.estimator.train_and_evaluate(mnist_classifier, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = mnist_classifier.evaluate(input_fn=lambda:input_fn(eval_data, eval_labels))\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = mnist_classifier.predict(input_fn=lambda:predict_input_fn(eval_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(pred_results))\n",
    "print(eval_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Save and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_receiver_input_fn():\n",
    "    \n",
    "    # serialized tf example - assumes that input_images is a 4-D tensor of shape [batch, height, width, channels]\n",
    "    input_images = tf.placeholder(dtype=tf.float32, shape=[None,None,None,1])\n",
    "    # resize_images needs that the input tensor has the channel dimension\n",
    "    images = tf.image.resize_images(input_images, [28,28])\n",
    "    # dictionary passed to the model \n",
    "    features = {'x': images}\n",
    "    # dictionary of serving input data \n",
    "    receiver_tensors = {'x': input_images}\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier.export_savedmodel(SAVE_DIR, serving_input_receiver_fn=serving_receiver_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdirs = [x for x in Path(SAVE_DIR).iterdir() if x.is_dir() and 'temp' not in str(x)]\n",
    "latest_dir = str(sorted(subdirs)[-1])\n",
    "predict_fn = predictor.from_saved_model(latest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serving images needs to be of shape [batch, height, width, channels]\n",
    "serving_examples = eval_data[0:2].reshape(-1,28,28,1)\n",
    "predictions = predict_fn({'x': serving_examples})\n",
    "print('predictions', predictions['classes'])\n",
    "print('true labels', eval_labels[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
